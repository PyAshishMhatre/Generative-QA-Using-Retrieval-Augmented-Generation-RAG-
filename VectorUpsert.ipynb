{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd12267ac2bf4c63ba2996f4dc72c711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01e75aad825d4cb280718730a3f7e5de",
              "IPY_MODEL_de21e55126a54f89b30c33ffc8dfe5d6",
              "IPY_MODEL_59ee5cb619404e5a9894f5c613bd8013"
            ],
            "layout": "IPY_MODEL_2d0d946e66c84f49bdec39f73bc30d87"
          }
        },
        "01e75aad825d4cb280718730a3f7e5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b28bb620654d5190b42de17868229b",
            "placeholder": "​",
            "style": "IPY_MODEL_5c34bc0f43ba477186a5e7001c7b653f",
            "value": "100%"
          }
        },
        "de21e55126a54f89b30c33ffc8dfe5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbb2099369454a698f423b85fdb232b0",
            "max": 177,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd349f41022f4f39ae6efdd86122ff71",
            "value": 177
          }
        },
        "59ee5cb619404e5a9894f5c613bd8013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3344890dfeb496ea8059456f8bf2a8c",
            "placeholder": "​",
            "style": "IPY_MODEL_41c2e3b2177e46b9bfd989a50bcc7e13",
            "value": " 177/177 [02:01&lt;00:00,  1.53s/it]"
          }
        },
        "2d0d946e66c84f49bdec39f73bc30d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b28bb620654d5190b42de17868229b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c34bc0f43ba477186a5e7001c7b653f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbb2099369454a698f423b85fdb232b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd349f41022f4f39ae6efdd86122ff71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3344890dfeb496ea8059456f8bf2a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c2e3b2177e46b9bfd989a50bcc7e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PyAshishMhatre/Generative-QA-Using-Retrieval-Augmented-Generation-RAG-/blob/main/VectorUpsert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Libraries\n",
        "!pip install pytube -q\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "\n",
        "# Function to download video and get transcriptions\n",
        "def get_transcriptions(url):\n",
        "    yt_video = YouTube(url)\n",
        "    stream = yt_video.streams.filter(only_audio=True)\n",
        "    stream = stream.first()\n",
        "    stream.download(filename=\"test.mp4\")\n",
        "\n",
        "    model = whisper.load_model('base')\n",
        "    output = model.transcribe(\"test.mp4\")\n",
        "    return output[\"text\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "59nnMgQERTWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd07f619-4b81-4034-e20b-84dbc6e4ad49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of 5 video URLs\n",
        "video_urls = [\"https://www.youtube.com/watch?v=t8rPDiGmwys\",\"https://www.youtube.com/watch?v=8ssQWJdsWIU&t=731s\",\n",
        "\"https://www.youtube.com/watch?v=qYFmTP2-k94\"]\n",
        "\n",
        "transcription = []  # list to store transcriptions\n",
        "\n",
        "# Loop through each video URL and get transcriptions\n",
        "for i, url in enumerate(video_urls):\n",
        "    transcriptions = get_transcriptions(url)\n",
        "    transcription.append(transcriptions)\n",
        "\n",
        "# Print the transcriptions list length\n",
        "print(len(transcription))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSU7yfPTbLQa",
        "outputId": "37624105-ce19-4928-b85a-ce0fec860e83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the long transcribe into short text\n",
        "\n",
        "short_text = []\n",
        "\n",
        "for i in transcription:\n",
        "  if len(i) >= 300:\n",
        "    for j in range(0,len(i),300):\n",
        "      short_text.append(i[j:j+300])\n",
        "  else:\n",
        "    short_text.append(i)"
      ],
      "metadata": {
        "id": "eL9lBrdziRZt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(short_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38F3STm9rtQ7",
        "outputId": "06e249c0-090b-4cb6-bcee-b9d166ba0bfc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai pinecone-client datasets -q\n",
        "import openai\n",
        "\n",
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = \"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\""
      ],
      "metadata": {
        "id": "IBpLbIfFNRlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d480fc51-54c3-4b26-80c6-cc7fead39613"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how is the taste of myprotein chocolate brownie whey protein\"\n",
        "\n",
        "# now query text-davinci-003 WITHOUT context\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=query,\n",
        "    temperature=0,\n",
        "    max_tokens=400,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop=None\n",
        ")\n",
        "\n",
        "res['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bLYuOsVYOhic",
        "outputId": "1dc056db-5638-4930-e21e-b193fef823ea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The taste of Myprotein Chocolate Brownie Whey Protein is generally described as being sweet and chocolatey with a hint of brownie flavor. Many people find it to be a delicious and satisfying protein shake.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query davinci model\n",
        "\n",
        "def complete(prompt):\n",
        "    # query text-davinci-003\n",
        "    res = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=400,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=None\n",
        "    )\n",
        "    return res['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "id": "97ddES4MQT1L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = (\n",
        "    \"Which training method should I use for sentence transformers when \" +\n",
        "    \"I only have pairs of related sentences?\"\n",
        ")\n",
        "\n",
        "complete(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "e5S3GPpOQXE2",
        "outputId": "ff2ef50a-f2b1-43f0-e562-0149f9230ade"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'If you only have pairs of related sentences, then the best training method to use for sentence transformers is the supervised learning approach. This approach involves providing the model with labeled data, such as pairs of related sentences, and then training the model to learn the relationships between the sentences. This approach is often used for tasks such as natural language inference, semantic similarity, and paraphrase identification.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding of the query which would be used to query the pinecone database\n",
        "# Here we are using the text-embedding-ada-002 model from open ai\n",
        "\n",
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"Sample document text goes here\"\n",
        "    ], engine=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "QKMSaJgmQaHu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res['data'][0]['embedding']"
      ],
      "metadata": {
        "id": "VkmwhunlUa3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup pinecone environment\n",
        "\n",
        "import pinecone\n",
        "\n",
        "index_name = 'openai-youtube-transcriptions'\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "pinecone.init(api_key=\"668b8cf1-087e-4cd0-b1f6-2992b5aeb80e\",\n",
        "              environment=\"northamerica-northeast1-gcp\"  # may be different, check at app.pinecone.io\n",
        "             )\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='cosine',\n",
        "        # metadata_config={'indexed': ['index']}\n",
        "    )\n",
        "  \n",
        "# connect to index\n",
        "index = pinecone.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1451-u-Qg5s",
        "outputId": "2f8ce3d7-8719-4a8a-e469-7a7cdde86f11"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make upsert list which includes the id, vector and actual text in the meta data\n",
        "# This text will be used to build the prompt to query the davinci model \n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import datetime\n",
        "from time import sleep\n",
        "\n",
        "upsert_list = []\n",
        "for i in tqdm(range(0, len(short_text))):\n",
        "\n",
        "    texts = short_text[i]\n",
        "    # create embeddings (try-except added to avoid RateLimitError)\n",
        "    try:\n",
        "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            sleep(5)\n",
        "            try:\n",
        "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = res['data'][0]['embedding']\n",
        "    metadata = {'text' : texts}\n",
        "    upsert_list.append((str(i), embeds, metadata)) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dd12267ac2bf4c63ba2996f4dc72c711",
            "01e75aad825d4cb280718730a3f7e5de",
            "de21e55126a54f89b30c33ffc8dfe5d6",
            "59ee5cb619404e5a9894f5c613bd8013",
            "2d0d946e66c84f49bdec39f73bc30d87",
            "04b28bb620654d5190b42de17868229b",
            "5c34bc0f43ba477186a5e7001c7b653f",
            "dbb2099369454a698f423b85fdb232b0",
            "fd349f41022f4f39ae6efdd86122ff71",
            "e3344890dfeb496ea8059456f8bf2a8c",
            "41c2e3b2177e46b9bfd989a50bcc7e13"
          ]
        },
        "id": "7kYIt6b2bmx1",
        "outputId": "6e30114d-cf2f-48c7-82cb-e07c8dd199be"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/177 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd12267ac2bf4c63ba2996f4dc72c711"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload vectors\n",
        "index.upsert(upsert_list)"
      ],
      "metadata": {
        "id": "cMmMvICHniu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ab4bc7-8f26-4958-a80e-81ac23a2f5bb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 177}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the process without any function calls\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\"how is the taste of myprotein chocolate brownie whey protein\"],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=4, include_metadata=True)"
      ],
      "metadata": {
        "id": "xMbVgF7Q4Mnw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "NVApnu6D-RQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d7a68b-3254-4126-f031-d3097306b137"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matches': [{'id': '164',\n",
              "              'metadata': {'text': 'y should go. These are the best '\n",
              "                                   \"experiences we've had with my protein \"\n",
              "                                   'products. Thank you, my protein for '\n",
              "                                   \"creating them. And we're gonna kickstart \"\n",
              "                                   'it off with brownie flavoured impact '\n",
              "                                   \"protein powder. This is coated. Oh, it's \"\n",
              "                                   \"coated, bro. And it's coated because it's \"\n",
              "                                   \"so simple. You can't go wrong, th\"},\n",
              "              'score': 0.857508898,\n",
              "              'values': []},\n",
              "             {'id': '98',\n",
              "              'metadata': {'text': 'h just chocolate protein powders. The '\n",
              "                                   'chocolate brownie just smells better. It '\n",
              "                                   'smells more chocolatey. I would say this '\n",
              "                                   'tastes kind of like the chocolate in those '\n",
              "                                   'snack pack pudding cups, those single '\n",
              "                                   'served chocolate pudding cups. This gives '\n",
              "                                   'me more of those vibes in the chocolate '\n",
              "                                   'brownie. It gives'},\n",
              "              'score': 0.847052872,\n",
              "              'values': []},\n",
              "             {'id': '96',\n",
              "              'metadata': {'text': 'ike chocolate. So yeah, if you want a '\n",
              "                                   'chocolate protein powder, solid option. '\n",
              "                                   'Chocolate brownie. Next up is chocolate '\n",
              "                                   \"smooth. That's what it looks like from the \"\n",
              "                                   'inside. It is a little darker than the '\n",
              "                                   'chocolate brownie. Chocolate smooth and '\n",
              "                                   'chocolate brownie. They taste basically '\n",
              "                                   'identical, but the c'},\n",
              "              'score': 0.845209,\n",
              "              'values': []},\n",
              "             {'id': '97',\n",
              "              'metadata': {'text': 'hocolate brownie smells a little bit '\n",
              "                                   'better. If you want a chocolate protein '\n",
              "                                   'powder, I would go with chocolate brownie. '\n",
              "                                   \"But if that's not available and you only \"\n",
              "                                   'have chocolate smooth, just get chocolate '\n",
              "                                   \"smooth because it's going to be basically \"\n",
              "                                   'the same thing. It tastes exactly the '\n",
              "                                   \"same. They're bot\"},\n",
              "              'score': 0.842879176,\n",
              "              'values': []}],\n",
              " 'namespace': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve function takes the query as input convert the input query into vector embeddings\n",
        "# This embeddings are used to do sematic search against the vectors in pinecone\n",
        "# In reponse we get 4 similar text which we combine with prompt start and prompt end to build the complete context for query\n",
        "# Finally the prompt is returned which is passed as input to the complete function for davinci model\n",
        "\n",
        "def retrieve(query):\n",
        "    limit = 3750\n",
        "    res = openai.Embedding.create(\n",
        "        input=[query],\n",
        "        engine=embed_model\n",
        "    )\n",
        "\n",
        "    # retrieve from Pinecone\n",
        "    xq = res['data'][0]['embedding']\n",
        "\n",
        "    # get relevant contexts\n",
        "    res = index.query(xq, top_k=4, include_metadata=True)\n",
        "    contexts = [x['metadata']['text'] for x in res['matches']]\n",
        "\n",
        "\n",
        "    # build our prompt with the retrieved contexts included\n",
        "    prompt_start = (\n",
        "        \"Answer the question based on the context below.\\n\\n\"+\n",
        "        \"Context:\\n\"\n",
        "    )\n",
        "    prompt_end = (\n",
        "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    # append contexts until hitting limit\n",
        "    for i in range(1, len(contexts)):\n",
        "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
        "            prompt = (\n",
        "                prompt_start +\n",
        "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
        "                prompt_end\n",
        "            )\n",
        "            break\n",
        "        elif i == len(contexts)-1:\n",
        "            prompt = (\n",
        "                prompt_start +\n",
        "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
        "                prompt_end\n",
        "            )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "leu6y4Ed-SHn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_with_contexts = retrieve(\"how is the taste of myprotein chocolate brownie whey protein\")\n",
        "print(query_with_contexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuhLzLFp_hT0",
        "outputId": "bac73116-11a3-48fc-c3a7-56dea85d3d0f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below.\n",
            "\n",
            "Context:\n",
            "y should go. These are the best experiences we've had with my protein products. Thank you, my protein for creating them. And we're gonna kickstart it off with brownie flavoured impact protein powder. This is coated. Oh, it's coated, bro. And it's coated because it's so simple. You can't go wrong, th\n",
            "\n",
            "---\n",
            "\n",
            "h just chocolate protein powders. The chocolate brownie just smells better. It smells more chocolatey. I would say this tastes kind of like the chocolate in those snack pack pudding cups, those single served chocolate pudding cups. This gives me more of those vibes in the chocolate brownie. It gives\n",
            "\n",
            "---\n",
            "\n",
            "ike chocolate. So yeah, if you want a chocolate protein powder, solid option. Chocolate brownie. Next up is chocolate smooth. That's what it looks like from the inside. It is a little darker than the chocolate brownie. Chocolate smooth and chocolate brownie. They taste basically identical, but the c\n",
            "\n",
            "---\n",
            "\n",
            "hocolate brownie smells a little bit better. If you want a chocolate protein powder, I would go with chocolate brownie. But if that's not available and you only have chocolate smooth, just get chocolate smooth because it's going to be basically the same thing. It tastes exactly the same. They're bot\n",
            "\n",
            "Question: how is the taste of myprotein chocolate brownie whey protein\n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final output of model with context query\n",
        "\n",
        "complete(query_with_contexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1GsnQsWK_g-n",
        "outputId": "805cacde-2fa4-4c88-8d83-abd62f894963"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The taste of myprotein chocolate brownie whey protein is similar to the chocolate in snack pack pudding cups and it smells better than the chocolate smooth whey protein.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xP5QS0wOEWhG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}